{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn, Tensor\n",
    "from datasets import load_from_disk\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
    "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_accumulation_steps = 1\n",
    "\n",
    "accelerator = Accelerator(gradient_accumulation_steps=gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"/mnt/data96/ujan/flow/data/wikitext-103-v1\"\n",
    "\n",
    "raw_datasets = load_from_disk(dataset_dir)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 1024  # get from sonar?\n",
    "\n",
    "# encoder\n",
    "t2vec_model = TextToEmbeddingModelPipeline(\n",
    "    encoder=\"text_sonar_basic_encoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\"\n",
    ")\n",
    "# decoder\n",
    "vec2text_model = EmbeddingToTextModelPipeline(\n",
    "    decoder=\"text_sonar_basic_decoder\",\n",
    "    tokenizer=\"text_sonar_basic_encoder\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define flow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMLP(nn.Module):\n",
    "    def __init__(self, dim: int = embed_dim, h: int = 2*embed_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim + 1, h), nn.ELU(),\n",
    "            nn.Linear(h, h), nn.ELU(),\n",
    "            nn.Linear(h, h), nn.ELU(),\n",
    "            nn.Linear(h, dim))\n",
    "    \n",
    "    def forward(self, t: Tensor, x_t: Tensor) -> Tensor:\n",
    "        return self.net(torch.cat((t, x_t), -1))\n",
    "    \n",
    "    def step(self, x_t: Tensor, t_start: Tensor, t_end: Tensor) -> Tensor:\n",
    "        t_start = t_start.view(1, 1).expand(x_t.shape[0], 1)\n",
    "        # midpoint ODE solver\n",
    "        # check for sonar embeddings\n",
    "        return x_t + (t_end - t_start) * self(t=t_start + (t_end - t_start) / 2, x_t= x_t + self(x_t=x_t, t=t_start) * (t_end - t_start) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_words = 20\n",
    "\n",
    "def get_paras(examples):\n",
    "    # tokenize and check length\n",
    "    examples['text'] = [x for x in examples['text'] if len(x.split())>min_words]\n",
    "    return examples\n",
    "\n",
    "# remove short strings\n",
    "with accelerator.main_process_first():\n",
    "    raw_datasets = raw_datasets.map(\n",
    "        get_paras,\n",
    "        batched=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sonar_embed(examples):\n",
    "    embeddings = t2vec_model.predict(examples['text'], source_lang=\"eng_Latn\")\n",
    "    examples['embeddings'] = embeddings\n",
    "    return examples\n",
    "\n",
    "# need to embed text on the fly\n",
    "with accelerator.main_process_first():\n",
    "    embedded_val = raw_datasets['validation'].map(\n",
    "    #embedded_datasets = raw_datasets.map(\n",
    "        sonar_embed,\n",
    "        batched=True,\n",
    "        batch_size=4,\n",
    "    )\n",
    "# embeddings are lists after map\n",
    "e = torch.FloatTensor(embedded_val[0]['embeddings']).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Homarus gammarus , known as the European lobster or common lobster , is a species of clawed lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into planktonic larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = embedded_val[0]['text']\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Homarus gammarus , known as European lobster or common lobster , is a common species of lobster native to the eastern Atlantic Ocean , the Mediterranean Sea , and parts of the Black Sea . It is closely related to the American homarus . It grows to a length of 60 cm (64 in) and weighs 25 grams , and has four white spots . Its appearance is somewhat rough . In the summer , lobsters can only be bred as \" lobsters \" , reproducing naturally .']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec2text_model.predict(e, target_lang=\"eng_Latn\", max_seq_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian noise to text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = FlowMLP()\n",
    "\n",
    "optimizer = torch.optim.Adam(flow.parameters(), 1e-2)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# 1 sample\n",
    "# t -> batch_size\n",
    "# samples -> batch_size, num_dim\n",
    "x_1 = e\n",
    "x_0 = torch.randn_like(e)\n",
    "\n",
    "t = torch.rand(len(e), 1)\n",
    "\n",
    "x_t = (1 - t) * x_0 + t * x_1\n",
    "dx_t = x_1 - x_0\n",
    "\n",
    "optimizer.zero_grad()\n",
    "loss_fn(flow(t=t, x_t=x_t), dx_t).backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec']\n",
      "\n",
      "['ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec ec']\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[32m      7\u001b[39m     x = flow.step(x_t=x, t_start=time_steps[i], t_end=time_steps[i + \u001b[32m1\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mvec2text_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meng_Latn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/sonar/inference_pipelines/text.py:344\u001b[39m, in \u001b[36mEmbeddingToTextModelPipeline.predict\u001b[39m\u001b[34m(self, inputs, target_lang, batch_size, progress_bar, sampler, **generator_kwargs)\u001b[39m\n\u001b[32m    342\u001b[39m     pipeline = add_progress_bar(pipeline, inputs=inputs, batch_size=batch_size)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m precision_context(\u001b[38;5;28mself\u001b[39m.model.dtype):\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     results: List[List[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [x \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m y]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/sonar/inference_pipelines/text.py:330\u001b[39m, in \u001b[36mEmbeddingToTextModelPipeline.predict.<locals>._do_translate\u001b[39m\u001b[34m(src_tensors)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_do_translate\u001b[39m(src_tensors: List[torch.Tensor]) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     texts, _ = \u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    332\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m texts\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/fairseq2/generation/text.py:119\u001b[39m, in \u001b[36mSequenceToTextConverter.batch_convert\u001b[39m\u001b[34m(self, source_seqs, source_padding_mask)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(source_seqs) == \u001b[32m0\u001b[39m:\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    116\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`source_seqs` must contain at least one element, but is empty instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/fairseq2/generation/text.py:145\u001b[39m, in \u001b[36mSequenceToTextConverter._do_convert\u001b[39m\u001b[34m(self, source_seqs, source_padding_mask)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# (S) -> (N, S)\u001b[39;00m\n\u001b[32m    143\u001b[39m target_prefix_seqs = \u001b[38;5;28mself\u001b[39m._target_prefix_seq.expand(batch_size, -\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m generator_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_prefix_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    147\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, hypotheses \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generator_output.hypotheses):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/fairseq2/generation/_beam_search/_generator.py:366\u001b[39m, in \u001b[36mBeamSearchSeq2SeqGenerator.__call__\u001b[39m\u001b[34m(self, source_seqs, source_padding_mask, prompt_seqs, prompt_padding_mask)\u001b[39m\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    341\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`min_gen_len` must be less than or equal to `max_gen_len` (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_gen_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m), but is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._min_gen_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instead. Adjust your `max_gen_len` argument.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    342\u001b[39m     )\n\u001b[32m    344\u001b[39m op = _BeamSearchSeq2SeqGeneratorOp(\n\u001b[32m    345\u001b[39m     \u001b[38;5;28mself\u001b[39m._model,\n\u001b[32m    346\u001b[39m     encoder_output,\n\u001b[32m   (...)\u001b[39m\u001b[32m    363\u001b[39m     \u001b[38;5;28mself\u001b[39m._step_hooks,\n\u001b[32m    364\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m hypotheses, counters = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Seq2SeqGeneratorOutput(\n\u001b[32m    369\u001b[39m     hypotheses, encoder_output, encoder_padding_mask, counters\n\u001b[32m    370\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/fairseq2/generation/_beam_search/_generator.py:524\u001b[39m, in \u001b[36m_AbstractBeamSearchSequenceGeneratorOp.__call__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    521\u001b[39m watch = Stopwatch(start=\u001b[38;5;28;01mTrue\u001b[39;00m, device=\u001b[38;5;28mself\u001b[39m._seqs.device)\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._step_nr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._min_prompt_len, \u001b[38;5;28mself\u001b[39m._max_seq_len):\n\u001b[32m--> \u001b[39m\u001b[32m524\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    525\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[38;5;28mself\u001b[39m._counters.generation_time = watch.get_elapsed_time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/fairseq2/generation/_beam_search/_generator.py:605\u001b[39m, in \u001b[36m_AbstractBeamSearchSequenceGeneratorOp._step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m:\n\u001b[32m    604\u001b[39m     \u001b[38;5;66;03m# Generate the next step output.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m     model_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_seqs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_nr\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_nr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28mself\u001b[39m._state_bag.increment_step_nr()\n\u001b[32m    609\u001b[39m     \u001b[38;5;28mself\u001b[39m._counters.num_generated_elements += \u001b[38;5;28mself\u001b[39m._seqs.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/fairseq2/generation/_beam_search/_generator.py:963\u001b[39m, in \u001b[36m_BeamSearchSeq2SeqGeneratorOp._decode\u001b[39m\u001b[34m(self, seqs)\u001b[39m\n\u001b[32m    953\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_decode\u001b[39m(\u001b[38;5;28mself\u001b[39m, seqs: Tensor) -> SequenceModelOutput:\n\u001b[32m    955\u001b[39m     decoder_output, decoder_padding_mask = \u001b[38;5;28mself\u001b[39m._model.decode(\n\u001b[32m    956\u001b[39m         seqs,\n\u001b[32m    957\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# We never use PAD in incremental decoding.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    960\u001b[39m         state_bag=\u001b[38;5;28mself\u001b[39m._state_bag,\n\u001b[32m    961\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/sonar/models/sonar_translation/model.py:78\u001b[39m, in \u001b[36mSonarEncoderDecoderModel.project\u001b[39m\u001b[34m(self, decoder_output, decoder_padding_mask)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mproject\u001b[39m(\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m, decoder_output: Tensor, decoder_padding_mask: Optional[PaddingMask]\n\u001b[32m     77\u001b[39m ) -> SequenceModelOutput:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_padding_mask\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/sonar/nn/conditional_decoder_model.py:92\u001b[39m, in \u001b[36mConditionalTransformerDecoderModel.project\u001b[39m\u001b[34m(self, decoder_output, decoder_padding_mask)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mproject\u001b[39m(\n\u001b[32m     89\u001b[39m     \u001b[38;5;28mself\u001b[39m, decoder_output: Tensor, decoder_padding_mask: Optional[PaddingMask]\n\u001b[32m     90\u001b[39m ) -> SequenceModelOutput:\n\u001b[32m     91\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Projection is exactly the same as with fairseq2 TransformerModel\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfinal_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SequenceModelOutput(logits, pad_idx=\u001b[38;5;28mself\u001b[39m.target_vocab_info.pad_idx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/flow/.venv/lib/python3.12/site-packages/fairseq2/nn/_projection.py:566\u001b[39m, in \u001b[36mTiedProjection.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "x = torch.randn_like(e)\n",
    "\n",
    "n_steps = 8\n",
    "time_steps = torch.linspace(0, 1.0, n_steps + 1)\n",
    "\n",
    "for i in range(n_steps):\n",
    "    x = flow.step(x_t=x, t_start=time_steps[i], t_end=time_steps[i + 1])\n",
    "    print(vec2text_model.predict(x, target_lang=\"eng_Latn\", max_seq_len=512))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
